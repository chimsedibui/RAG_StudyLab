{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Package Installation and Imports\n",
        "\n",
        "The cell below installs all necessary packages required to run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "from llama_index.core.evaluation import (\n",
        "    DatasetGenerator,\n",
        "    FaithfulnessEvaluator,\n",
        "    RelevancyEvaluator\n",
        ")\n",
        "from llama_index.core import Settings\n",
        "from langchain_groq import chat_models, ChatGroq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(id_='b547800a-f986-4e4d-a72c-0787eb18bad9', embedding=None, metadata={'page_label': '1', 'file_name': '2303.08774v6.pdf', 'file_path': '/data_hdd_16t/khanhtran/LLM/RAG/Data/5.Choose_chunksize_data/2303.08774v6.pdf', 'file_type': 'application/pdf', 'file_size': 5245564, 'creation_date': '2025-10-16', 'last_modified_date': '2025-10-16'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\\nThis contrasts with GPT-3.5, which scores in the bottom 10%.\\nOn a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\\nand most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\\nOn the MMLU benchmark [35, 36], an English-language suite of multiple-choice questions covering\\n57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\\nalso demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4\\nsurpasses the English-language state-of-the-art in 24 of 26 languages considered. We discuss these\\nmodel capability results, as well as model safety improvements and results, in more detail in later\\nsections.\\nThis report also discusses a key challenge of the project, developing deep learning infrastructure and\\noptimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the final run to increase confidence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [1, 37, 38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n∗Please cite this work as “OpenAI (2023)\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.com\\narXiv:2303.08774v6  [cs.CL]  4 Mar 2024', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_dir = \"/data_hdd_16t/khanhtran/LLM/RAG/Data/5.Choose_chunksize_data\"\n",
        "documents = SimpleDirectoryReader(data_dir).load_data(num_workers=1)\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create evaluation questions and pick k out of them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data_hdd_16t/khanhtran/anaconda/envs/workdir/lib/python3.10/site-packages/llama_index/core/evaluation/dataset_generation.py:201: DeprecationWarning: Call to deprecated class DatasetGenerator. (Deprecated in favor of `RagDatasetGenerator` which should be used instead.)\n",
            "  return cls(\n",
            "2025-10-16 14:13:25,435 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-10-16 14:13:26,111 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-10-16 14:13:27,070 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-10-16 14:13:27,620 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-10-16 14:13:28,182 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-10-16 14:13:28,825 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "2025-10-16 14:13:29,509 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "/data_hdd_16t/khanhtran/anaconda/envs/workdir/lib/python3.10/site-packages/llama_index/core/evaluation/dataset_generation.py:297: DeprecationWarning: Call to deprecated class QueryResponseDataset. (Deprecated in favor of `LabelledRagDataset` which should be used instead.)\n",
            "  return QueryResponseDataset(queries=queries, responses=responses_dict)\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.prompts import PromptTemplate\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "import random   \n",
        "\n",
        "num_eval_questions = 15\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0)\n",
        "llm_wrapped = LangChainLLM(llm=llm)\n",
        "\n",
        "custom_question_prompt = PromptTemplate(\n",
        "    template=\"\"\"\n",
        "    You are a expert question generator. For the following context, \n",
        "    generate a diverse and detailed question that can be answered by the context to test understanding of the contents. \\n\n",
        "    Context: {{context_str}} \\n\n",
        "    Question:\n",
        "    \"\"\" \n",
        ")  \n",
        "\n",
        "data_generator = DatasetGenerator.from_documents(\n",
        "    documents[20:25], \n",
        "    llm=llm_wrapped,\n",
        "    text_question_template=custom_question_prompt\n",
        ")\n",
        "\n",
        "eval_questions = data_generator.generate_questions_from_nodes()\n",
        "k_eval_questions = random.sample(eval_questions, num_eval_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define metrics evaluators and modify llama_index faithfullness evaluator prompt to rely on the context "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "Settings.llm = llm\n",
        "faithfulness_llm = FaithfulnessEvaluator()\n",
        "\n",
        "faithfulness_new_prompt_template = PromptTemplate(\"\"\" Please tell if a given piece of information is directly supported by the context.\n",
        "    You need to answer with either YES or NO.\n",
        "    Answer YES if any part of the context explicitly supports the information, even if most of the context is unrelated. If the context does not explicitly support the information, answer NO. Some examples are provided below.\n",
        "\n",
        "    Information: Apple pie is generally double-crusted.\n",
        "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
        "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
        "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
        "    Answer: YES\n",
        "\n",
        "    Information: Apple pies taste bad.\n",
        "    Context: An apple pie is a fruit pie in which the principal filling ingredient is apples.\n",
        "    Apple pie is often served with whipped cream, ice cream ('apple pie à la mode'), custard, or cheddar cheese.\n",
        "    It is generally double-crusted, with pastry both above and below the filling; the upper crust may be solid or latticed (woven of crosswise strips).\n",
        "    Answer: NO\n",
        "\n",
        "    Information: Paris is the capital of France.\n",
        "    Context: This document describes a day trip in Paris. You will visit famous landmarks like the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
        "    Answer: NO\n",
        "\n",
        "    Information: {query_str}\n",
        "    Context: {context_str}\n",
        "    Answer:\n",
        "\n",
        "    \"\"\")\n",
        "\n",
        "faithfulness_llm.update_prompts({\"your_prompt_key\": faithfulness_new_prompt_template}) # Update the prompts dictionary with the new prompt template\n",
        "\n",
        "relevancy_llm = RelevancyEvaluator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to evaluate metrics for each chunk size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def evaluate_response_time_and_accuracy(chunk_size, eval_questions):\n",
        "    \"\"\"\n",
        "    Evaluate the average response time, faithfulness, and relevancy of responses generated by GPT-3.5-turbo for a given chunk size.\n",
        "    Parameters:\n",
        "    chunk_size (int): The size of data chunks being processed.\n",
        "    Returns:\n",
        "    tuple: A tuple containing the average response time, faithfulness, and relevancy metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    total_response_time: int = 0\n",
        "    total_faithfulness: int = 0\n",
        "    total_relevancy: int = 0\n",
        "\n",
        "    llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "\n",
        "    Settings.llm = llm\n",
        "    Settings.chunk_size = chunk_size\n",
        "    Settings.chunk_overlap = chunk_size // 5 \n",
        "\n",
        "    vector_index = VectorStoreIndex.from_documents(documents[15:20])\n",
        "    \n",
        "    query_engine = vector_index.as_query_engine(similarity_top_k=5)\n",
        "    num_questions = len(eval_questions)\n",
        "\n",
        "    for question in eval_questions:\n",
        "        start_time = time.time()\n",
        "        response_vector = query_engine.query(question)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        \n",
        "        time.sleep(random.uniform(1.5, 3.0))  \n",
        "        \n",
        "        faithfulness_result = faithfulness_llm.evaluate_response(\n",
        "            response=response_vector\n",
        "        ).passing\n",
        "        \n",
        "        time.sleep(random.uniform(1.5, 3.0))  \n",
        "        \n",
        "        relevancy_result = relevancy_llm.evaluate_response(\n",
        "            query=question, response=response_vector\n",
        "        ).passing\n",
        "\n",
        "        total_response_time += elapsed_time\n",
        "        total_faithfulness += faithfulness_result\n",
        "        total_relevancy += relevancy_result\n",
        "\n",
        "    average_response_time = total_response_time / num_questions\n",
        "    average_faithfulness = total_faithfulness / num_questions\n",
        "    average_relevancy = total_relevancy / num_questions\n",
        "\n",
        "    return average_response_time, average_faithfulness, average_relevancy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test different chunk sizes "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "Settings.embed_model = embed_model\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
        "\n",
        "chunk_sizes = [128]\n",
        "\n",
        "for chunk_size in chunk_sizes:\n",
        "  avg_response_time, avg_faithfulness, avg_relevancy = evaluate_response_time_and_accuracy(chunk_size, k_eval_questions)\n",
        "  print(f\"Chunk size {chunk_size} - Average Response time: {avg_response_time:.2f}s, Average Faithfulness: {avg_faithfulness:.2f}, Average Relevancy: {avg_relevancy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "workdir",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
